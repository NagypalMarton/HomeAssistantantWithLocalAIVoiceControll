# Home Assistant + Local LLM DevOps Project

Multi-user Home Assistant installation with locally running Ollama LLM models, powered by NVIDIA GPUs.

## ğŸ—ï¸ Architecture

This project provides a complete DevOps pipeline for running Home Assistant with Ollama LLM in a multi-user Kubernetes environment.

**Components:**
- **Home Assistant**: Smart home automation platform
- **Ollama**: Local LLM inference with NVIDIA GPU support
- **Kubernetes**: Container orchestration with GPU node pools
- **Helm**: Package management and multi-environment deployment
- **Terraform**: Infrastructure as Code for cluster provisioning
- **GitHub Actions**: CI/CD pipeline
- **Zabbix**: Monitoring and alerting

## ğŸ“‹ Prerequisites

### Local Development
- Docker & Docker Compose
- NVIDIA GPU with drivers (optional for dev)
- 16GB+ RAM recommended
- 100GB+ free disk space for models

### Production
- Kubernetes cluster (1.28+)
- NVIDIA GPU nodes (Tesla T4, V100, or better)
- kubectl & Helm 3
- Terraform 1.5+
- Domain name with DNS access

## ğŸš€ Quick Start

### Local Development

```bash
# Clone repository
git clone <repository-url>
cd HomeAssistantantWithLocalAIVoiceControll

# Setup local environment
chmod +x scripts/setup-local.sh
./scripts/setup-local.sh
```

Access services:
- Home Assistant: http://localhost:8123
- Zabbix: http://localhost:8080 (Admin/zabbix)
- Ollama API: http://localhost:11434

### Kubernetes Deployment

```bash
# Deploy to development
./scripts/deploy-k8s.sh dev

# Deploy to production
./scripts/deploy-k8s.sh prod
```

## ğŸ“š Documentation

Detailed documentation available in the `docs/` directory:

- [Setup Guide](docs/SETUP.md) - Installation and configuration
- [Deployment Guide](docs/DEPLOYMENT.md) - Kubernetes deployment
- [User Guide](docs/USER_GUIDE.md) - Multi-user provisioning
- [Architecture](docs/ARCHITECTURE.md) - System design and components

## ğŸ”§ Project Structure

```
.
â”œâ”€â”€ docker/                  # Docker configurations
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ home-assistant/
â”‚   â””â”€â”€ ollama/
â”œâ”€â”€ kubernetes/              # Raw Kubernetes manifests
â”‚   â”œâ”€â”€ namespaces/
â”‚   â”œâ”€â”€ home-assistant/
â”‚   â”œâ”€â”€ ollama/
â”‚   â””â”€â”€ ingress/
â”œâ”€â”€ helm/                    # Helm charts
â”‚   â””â”€â”€ ha-llm-stack/
â”œâ”€â”€ terraform/               # Infrastructure as Code
â”‚   â”œâ”€â”€ modules/
â”‚   â””â”€â”€ environments/
â”œâ”€â”€ .github/workflows/       # CI/CD pipelines
â”œâ”€â”€ monitoring/              # Zabbix configuration
â”‚   â””â”€â”€ zabbix/
â”œâ”€â”€ config/                  # Application configs
â”œâ”€â”€ scripts/                 # Utility scripts
â””â”€â”€ docs/                    # Documentation
```

## ğŸ” Security

- Store secrets in Kubernetes Secrets or external secret managers
- Enable RBAC for multi-user isolation
- Use TLS certificates (cert-manager)
- Configure network policies
- Regular security scans with Trivy

## ğŸ“Š Monitoring

Zabbix monitors:
- Home Assistant availability and response time
- Ollama API health and model availability
- GPU utilization and memory
- Kubernetes resource usage
- Custom alerts and dashboards

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“ License

See [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

For issues and questions:
- GitHub Issues: [Create an issue](../../issues)
- Documentation: [docs/](docs/)
- Community: [Discussions](../../discussions)

## ğŸ¯ Roadmap

- [ ] Multi-cloud Terraform modules (AWS, Azure, GCP)
- [ ] Advanced GPU sharing and scheduling
- [ ] Grafana dashboards
- [ ] Voice integration examples
- [ ] Model fine-tuning pipeline
- [ ] Backup and disaster recovery automation
- [ ] Cost optimization strategies

---

**Note**: This project requires NVIDIA GPUs for optimal Ollama performance. CPU-only mode is available but significantly slower.
