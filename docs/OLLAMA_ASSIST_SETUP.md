# Ollama Conversation Agent - HasznÃ¡lati ÃºtmutatÃ³

## âœ… Sikeresen telepÃ­tve!

A custom Ollama conversation agent betÃ¶ltÅ‘dÃ¶tt a Home Assistant-ba. Most mÃ¡r be tudod Ã¡llÃ­tani az Assist-ben.

## ğŸ“‹ LÃ©pÃ©sek az Ollama mint Assist Agent beÃ¡llÃ­tÃ¡sÃ¡hoz

### 1. Home Assistant megnyitÃ¡sa
Nyisd meg: **http://localhost:8123**

### 2. Ollama Integration hozzÃ¡adÃ¡sa

1. MenÃ¼ â†’ **BeÃ¡llÃ­tÃ¡sok (Settings)** â†’ **EszkÃ¶zÃ¶k Ã©s SzolgÃ¡ltatÃ¡sok (Devices & Services)**
2. Kattints a jobb alsÃ³ sarokban a **+ INTEGRÃCIÃ“ HOZZÃADÃSA** gombra
3. Keresd meg: **"Ollama Conversation"**
4. Add meg az alÃ¡bbi adatokat:
   - **Ollama Host URL**: `http://ollama:11434`
   - **Model neve**: `llama3.2:3b`
5. Kattints **KÃœLDÃ‰S (Submit)**

### 3. Ollama beÃ¡llÃ­tÃ¡sa mint alapÃ©rtelmezett Assist Agent

1. MenÃ¼ â†’ **BeÃ¡llÃ­tÃ¡sok (Settings)** â†’ **Hang Asszisztensek (Voice assistants)**
2. Kattints a **Home Assistant** asszisztensre
3. A **BeszÃ©lgetÃ©si Ã¼gynÃ¶k (Conversation agent)** menÃ¼pontban vÃ¡laszd ki: **Ollama**
4. Kattints **MENTÃ‰S (Save)**

### 4. Assist hasznÃ¡lata Ollama-val

**MÃ³dszer 1: Chat interfÃ©sz**
- Kattints a jobb felsÅ‘ sarokban a **mikrofonra** vagy **chat ikonra**
- Ãrj be kÃ©rdÃ©st magyarul: 
  - "Szia! Ki vagy?"
  - "Miben tudsz segÃ­teni?"
  - "Mi az idÅ‘jÃ¡rÃ¡s?"
- Az Ollama llama3.2:3b modell fog vÃ¡laszolni

**MÃ³dszer 2: Voice (opcionÃ¡lis)**
- Ha van mikrofonod, beszÃ©lj be
- Az Ollama fogja feldolgozni a szÃ¶veget Ã©s vÃ¡laszolni

**MÃ³dszer 3: Dashboard**
- MenÃ¼ â†’ **Ollama Chat** nÃ©zet
- HasznÃ¡ld a gyors gombokat vagy az input mezÅ‘t

## ğŸ” EllenÅ‘rzÃ©s

### Ollama agent stÃ¡tusz ellenÅ‘rzÃ©se:
```powershell
# Logok
docker logs homeassistant --tail=50 | findstr ollama

# Ollama API teszt
curl http://localhost:11434/api/generate -d "{\"model\":\"llama3.2:3b\",\"prompt\":\"Hello\",\"stream\":false}"
```

### Home Assistant elÃ©rhetÅ‘:
```powershell
(Invoke-WebRequest http://localhost:8123 -UseBasicParsing).StatusCode
# Should return: 200
```

## ğŸ¯ Mit fog csinÃ¡lni az Ollama Agent?

1. **Minden Assist kÃ©rdÃ©s** â†’ Ollama llama3.2:3b model dolgozza fel
2. **Magyar nyelven** vÃ¡laszol
3. **Smart home context** - okos otthon asszisztenskÃ©nt viselkedik
4. **Helyi futÃ¡s** - minden adat a gÃ©peden marad, nincs cloud

## âš™ï¸ TestreszabÃ¡s

### Model vÃ¡ltÃ¡sa:
1. BeÃ¡llÃ­tÃ¡sok â†’ EszkÃ¶zÃ¶k Ã©s SzolgÃ¡ltatÃ¡sok â†’ Ollama Conversation
2. Kattints **CONFIGURE**
3. VÃ¡lts modelt: `phi3:mini` vagy `qwen2.5:3b`

### System prompt mÃ³dosÃ­tÃ¡sa:
Szerkeszd: `custom_components/ollama_conversation/conversation.py` fÃ¡jlban a prompt-ot

### TovÃ¡bbi modellek telepÃ­tÃ©se:
```powershell
docker exec ollama ollama pull phi3:mini
docker exec ollama ollama pull qwen2.5:3b
```

## ğŸ› HibaelhÃ¡rÃ­tÃ¡s

### "Ollama Conversation" nem jelenik meg az integrÃ¡ciÃ³k kÃ¶zÃ¶tt:
```powershell
# ÃšjraindÃ­tÃ¡s
docker restart homeassistant

# Logok ellenÅ‘rzÃ©se
docker logs homeassistant 2>&1 | findstr "ollama_conversation"
```

### "Cannot connect to Ollama":
```powershell
# Ollama stÃ¡tusz
docker ps | findstr ollama

# Ollama ÃºjraindÃ­tÃ¡sa
docker restart ollama
```

### VÃ¡lasz tÃºl lassÃº:
- CsÃ¶kkentsd a `num_predict` Ã©rtÃ©ket (150 â†’ 100)
- VÃ¡ltsd kisebb modellre: `phi3:mini`

## ğŸ“‚ FÃ¡jlok

- **Custom component**: `config/home-assistant/custom_components/ollama_conversation/`
- **Configuration**: `config/home-assistant/configuration.yaml`
- **Dashboard**: `config/home-assistant/ui-lovelace.yaml`
- **Scripts**: `config/home-assistant/scripts.yaml`

## ğŸ‰ Sikeres hasznÃ¡lat jele

Ha minden mÅ±kÃ¶dik:
1. Assist chat-ben Ã­rsz egy kÃ©rdÃ©st magyarul
2. 2-5 mÃ¡sodperc mÃºlva Ã©rtelmes vÃ¡laszt kapsz magyarul
3. A vÃ¡lasz kontextusÃ¡ban Ã©rthetÅ‘ Ã©s hasznos
4. Logokban nincs ERROR az ollama_conversation-nÃ©l

Most mÃ¡r **chatelhetsz az Assist-tel Ollama segÃ­tsÃ©gÃ©vel**! ğŸš€
